import os
import re
import json
import time
import csv
import requests
import logging
from urllib.parse import urlparse
from typing import List, Optional, Dict
from dataclasses import dataclass
from bs4 import BeautifulSoup
from tqdm import tqdm
import warnings
import argparse

# C√†i ƒë·∫∑t ƒë·ªÉ b·ªè qua c·∫£nh b√°o SSL
warnings.filterwarnings("ignore", category=requests.packages.urllib3.exceptions.InsecureRequestWarning)

# ===============================
# Logger setup
# ===============================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# ===============================
# Data model
# ===============================
@dataclass
class UrlRecord:
    product_id: str
    url: str
    source_collection: str

# ===============================
# Core functions
# ===============================
def http_get(url: str) -> Optional[str]:
    """Fetch raw HTML."""
    try:
        resp = requests.get(url, timeout=15, verify=False, headers={
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        })
        if resp.status_code == 200 and resp.text.strip():
            return resp.text
    except Exception as e:
        logger.warning(f"HTTP error for {url}: {e}")
    return None


def extract_product_name(html: str) -> Optional[str]:
    """Extract product name from JSON-LD or fallback tags."""
    try:
        soup = BeautifulSoup(html, "html.parser")

        # Try JSON-LD first
        for script in soup.find_all("script", type="application/ld+json"):
            try:
                data = json.loads(script.string)
                # JSON-LD c√≥ th·ªÉ ch·ª©a list
                if isinstance(data, list):
                    for item in data:
                        if isinstance(item, dict) and item.get("@type") == "Product":
                            name = item.get("name")
                            if name:
                                return name.strip()
                elif isinstance(data, dict) and data.get("@type") == "Product":
                    name = data.get("name")
                    if name:
                        return name.strip()
            except Exception:
                continue

        # Fallback 1: <meta property="og:title">
        meta_title = soup.find("meta", property="og:title")
        if meta_title and meta_title.get("content"):
            return meta_title["content"].strip()

        # Fallback 2: <title>
        if soup.title and soup.title.text:
            return soup.title.text.strip()

        # Fallback 3: H1
        h1 = soup.find("h1")
        if h1 and h1.text.strip():
            return h1.text.strip()

    except Exception as e:
        logger.warning(f"extract_product_name error: {e}")
    return None


def _derive_name_from_slug(url: str) -> Optional[str]:
    """Derive product name from URL slug as a last resort."""
    parsed = urlparse(url)
    slug = parsed.path.rsplit('/', 1)[-1]
    slug = slug.split('.html', 1)[0]
    # T√°ch slug b·∫±ng d·∫•u g·∫°ch ngang/d·∫•u g·∫°ch d∆∞·ªõi
    parts = [w for w in re.split(r"[-_]+", slug) if w]
    if not parts:
        return None
    # Gh√©p l·∫°i, ch·ªâ gi·ªØ l·∫°i c√°c t·ª´ l√† ch·ªØ c√°i, vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu
    name = " ".join(p.capitalize() for p in parts if p.isalpha())
    return name.strip() if name else None


def is_likely_product_url(url: str) -> bool:
    """T√¨nh tr·∫°ng l·ªçc URL hi·ªán t·∫°i c·ªßa b·∫°n."""
    return "glamira" in (url or "") and ".html" in (url or "")


def crawl_product_names_parallel(records: List[UrlRecord], debug_limit: int = 10) -> List[Dict]:
    """Crawl s·∫£n ph·∫©m v√† tr√≠ch xu·∫•t t√™n. ƒê√£ s·ª≠a l·ªói logic tr·∫°ng th√°i."""
    results = []
    for idx, rec in enumerate(tqdm(records, desc="Crawling products")):
        html = http_get(rec.url)
        product_name = None
        status = "no_html"
        
        if html:
            # 1. Th·ª≠ tr√≠ch xu·∫•t t·ª´ HTML/JSON-LD
            product_name = extract_product_name(html)
            
            if product_name:
                # ‚úÖ S·ª¨A L·ªñI: G√°n tr·∫°ng th√°i th√†nh c√¥ng khi t√¨m th·∫•y t√™n
                status = "success" 
            else:
                # 2. Th·ª≠ slug heuristic
                slug_name = _derive_name_from_slug(rec.url)
                if slug_name:
                    product_name = slug_name
                    status = "slug_heuristic"
                else:
                    status = "no_name_found"

            # L∆∞u debug HTML cho c√°c trang ƒë·∫ßu ti√™n
            if idx < debug_limit:
                host = urlparse(rec.url).netloc.replace(".", "_")
                fname = f"debug_html_{host}_{int(time.time())}_{rec.product_id}.html"
                with open(fname, "w", encoding="utf-8") as f:
                    f.write(html)
                logger.info(f"üß™ Saved debug HTML to {fname}")
                
        results.append({
            "product_id": rec.product_id,
            "product_name": product_name or "",
            "url": rec.url,
            "source_collection": rec.source_collection,
            "status": status,
            "fetched_at": int(time.time())
        })
        # Ghi log ti·∫øn tr√¨nh √≠t h∆°n ƒë·ªÉ tr√°nh qu√° t·∫£i console
        if (idx + 1) % 100 == 0 or idx + 1 == len(records):
            logger.info(f"‚è≥ Progress: {idx+1}/{len(records)} ({(idx+1)/len(records)*100:.1f}%)")
            
    return results


def append_candidates_jsonl(rows, path="product_name_candidates.jsonl"):
    with open(path, "a", encoding="utf-8") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")


def append_final_csv(rows, path="product_names_final.csv"):
    exists = os.path.exists(path)
    with open(path, "a", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=[
            "product_id", "product_name", "url", "source_collection", "status", "fetched_at"
        ])
        if not exists:
            writer.writeheader()
        writer.writerows(rows)


# ===============================
# CLI (ƒê√É S·ª¨A L·ªñI L·ªåC D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO)
# =================================================================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Crawl product names from URLs")
    parser.add_argument("--input", type=str, default="merged_data.csv", help="Input CSV file path")
    parser.add_argument("--n", type=int, default=20000, help="Max number of *distinct* products to crawl")
    args = parser.parse_args()

    # S·ª¨ D·ª§NG DICT ƒê·ªÇ GOM T·∫§T C·∫¢ URL V√Ä CH·ªåN URL T·ªêT NH·∫§T CHO M·ªñI PRODUCT_ID
    best_records: Dict[str, UrlRecord] = {} 
    total_rows_read = 0

    logger.info(f"Reading input data from {args.input}...")

    try:
        # ƒê·ªçc d·ªØ li·ªáu th√¥
        with open(args.input, "r", encoding="utf-8") as f:
            # L∆ØU √ù: ƒê·∫£m b·∫£o fieldnames kh·ªõp v·ªõi c·∫•u tr√∫c file CSV g·ªëc c·ªßa b·∫°n
            reader = csv.DictReader(f, fieldnames=["product_id", "url", "source_collection", "fetched_at"])
            
            # S·ª≠ d·ª•ng tqdm ƒë·ªÉ hi·ªÉn th·ªã ti·∫øn tr√¨nh ƒë·ªçc file
            for row in tqdm(reader, desc="Processing input data"):
                total_rows_read += 1
                pid = str(row.get("product_id")).strip()
                url = row.get("url")
                
                if not pid or not url:
                    continue

                # Logic ch·ªçn URL T·ªêT NH·∫§T cho m·ªói ID:
                is_good_url = is_likely_product_url(url)
                
                # A. N·∫øu ch∆∞a c√≥ b·∫£n ghi n√†o cho ID n√†y, ch·ªçn URL hi·ªán t·∫°i
                if pid not in best_records:
                    best_records[pid] = UrlRecord(pid, url, row.get("source_collection", "unknown"))
                
                # B. N·∫øu ƒë√£ c√≥ b·∫£n ghi, CH·ªà C·∫¨P NH·∫¨T n·∫øu URL M·ªöI T·ªêT H∆†N URL C≈®
                # (T·ª©c l√† URL hi·ªán t·∫°i l√† URL kh√¥ng h·ª£p l·ªá, nh∆∞ng URL m·ªõi h·ª£p l·ªá)
                elif is_good_url and not is_likely_product_url(best_records[pid].url):
                    best_records[pid] = UrlRecord(pid, url, row.get("source_collection", "unknown"))
                
                # D·ª´ng n·∫øu ƒë√£ ƒë·∫°t ƒë·∫øn gi·ªõi h·∫°n N (d√†nh cho ID duy nh·∫•t)
                if len(best_records) >= args.n:
                    break
        
    except Exception as e:
        logger.error(f"Failed to read input file {args.input}: {e}")
        exit(1)

    # L·∫•y danh s√°ch c√°c b·∫£n ghi t·ªët nh·∫•t ƒë·ªÉ crawl (ƒê√£ l·ªçc ID duy nh·∫•t)
    url_records = list(best_records.values())

    logger.info(f"Total rows read from input: {total_rows_read}")
    logger.info(f"Total input (distinct) products: {len(url_records)}") 
    
    # -----------------------------------------------------------------
    # B·∫ÆT ƒê·∫¶U CRAWL
    # -----------------------------------------------------------------
    logger.info(f"Starting crawl for {len(url_records)} URLs...")
    results = crawl_product_names_parallel(url_records)
    
    # X·ª≠ l√Ω k·∫øt qu·∫£
    append_candidates_jsonl(results)
    
    named_only = [r for r in results if r.get("product_name")]
    
    logger.info(f"Total successfully crawled product_name: {len(named_only)}")
    
    if len(named_only) < len(url_records):
        missed = [r['product_id'] for r in results if not r.get("product_name")]
        logger.warning(f"Products could not be named: {missed[:10]}... (Total: {len(missed)})")
    
    if named_only:
        append_final_csv(named_only)
        logger.info(f"‚úÖ Saved {len(named_only)} named products to product_names_final.csv.")
    else:
        logger.warning("‚ö†Ô∏è No product names extracted.")